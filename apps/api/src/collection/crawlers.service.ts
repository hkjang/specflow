import { Injectable, NotFoundException } from '@nestjs/common';
import { PrismaService } from '../prisma/prisma.service';
import { Prisma } from '@prisma/client';
import { Readability } from '@mozilla/readability';
import { JSDOM } from 'jsdom';
import RSSParser from 'rss-parser';
import { AiService } from '../ai/ai.service';

@Injectable()
export class CrawlersService {
    constructor(
        private prisma: PrismaService,
        private aiService: AiService
    ) { }

    create(data: Prisma.CrawlerCreateInput) {
        return this.prisma.crawler.create({ data });
    }

    findAll() {
        return this.prisma.crawler.findMany({
            include: {
                history: {
                    take: 5,
                    orderBy: { startedAt: 'desc' }
                }
            },
            orderBy: { createdAt: 'desc' }
        });
    }

    findOne(id: string) {
        return this.prisma.crawler.findUnique({ 
            where: { id },
            include: {
                history: {
                    take: 10,
                    orderBy: { startedAt: 'desc' }
                }
            }
        });
    }

    update(id: string, data: Prisma.CrawlerUpdateInput) {
        return this.prisma.crawler.update({ where: { id }, data });
    }

    async remove(id: string) {
        // Delete history first, then crawler
        await this.prisma.crawlHistory.deleteMany({ where: { crawlerId: id }});
        return this.prisma.crawler.delete({ where: { id } });
    }
    
    // --- History API ---
    async getHistory(crawlerId?: string, limit = 50) {
        return this.prisma.crawlHistory.findMany({
            where: crawlerId ? { crawlerId } : {},
            include: {
                crawler: {
                    select: { id: true, name: true, url: true, category: true }
                }
            },
            orderBy: { startedAt: 'desc' },
            take: limit
        });
    }
    
    async getHistoryStats() {
        const now = new Date();
        const oneDayAgo = new Date(now.getTime() - 24 * 60 * 60 * 1000);
        const oneWeekAgo = new Date(now.getTime() - 7 * 24 * 60 * 60 * 1000);
        
        const [todayRuns, weeklyRuns, totalSuccess, totalFailed] = await Promise.all([
            this.prisma.crawlHistory.count({ where: { startedAt: { gte: oneDayAgo } } }),
            this.prisma.crawlHistory.count({ where: { startedAt: { gte: oneWeekAgo } } }),
            this.prisma.crawlHistory.count({ where: { status: 'SUCCESS' } }),
            this.prisma.crawlHistory.count({ where: { status: 'FAILED' } })
        ]);
        
        // Get recent success rate
        const recentRuns = await this.prisma.crawlHistory.findMany({
            where: { startedAt: { gte: oneWeekAgo } },
            select: { status: true }
        });
        const successRate = recentRuns.length > 0 
            ? Math.round((recentRuns.filter(r => r.status === 'SUCCESS').length / recentRuns.length) * 100)
            : 0;
        
        return {
            todayRuns,
            weeklyRuns,
            totalSuccess,
            totalFailed,
            successRate
        };
    }
    
    // --- Collected Data API ---
    async getCollectedData(limit = 50) {
        return this.prisma.extractionSource.findMany({
            where: { type: 'URL' },
            orderBy: { createdAt: 'desc' },
            take: limit,
            select: {
                id: true,
                type: true,
                metadata: true,
                createdAt: true,
                content: false // Exclude content for list view
            }
        });
    }
    
    async getCollectedDataById(id: string) {
        return this.prisma.extractionSource.findUnique({
            where: { id },
            include: {
                jobs: { take: 5 },
                drafts: { take: 10 }
            }
        });
    }

    // --- Run Crawler (with History Recording) ---
    async runCrawler(id: string) {
        const crawler = await this.prisma.crawler.findUnique({ where: { id } });
        if (!crawler) throw new NotFoundException('Crawler not found');
        
        const startTime = new Date();
        let status = 'SUCCESS';
        let pagesFound = 0;
        let itemsExtracted = 0;
        let errorMessage: string | null = null;
        let duration = 0;
        let sourceId: string | null = null;
        
        try {
            console.log(`[CRAWL] Starting: ${crawler.name} @ ${crawler.url}`);
            
            // ==== REAL HTTP FETCH ====
            let realContent: string;
            try {
                const response = await fetch(crawler.url, {
                    headers: {
                        'User-Agent': 'SpecFlow-Crawler/1.0 (Requirement Extraction Bot)',
                        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                        'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8'
                    },
                    signal: AbortSignal.timeout(30000) // 30Ï¥à ÌÉÄÏûÑÏïÑÏõÉ
                });
                
                if (!response.ok) {
                    throw new Error(`HTTP ${response.status}: ${response.statusText}`);
                }
                
                realContent = await response.text();
                pagesFound = 1;
                console.log(`[CRAWL] Fetched ${realContent.length} bytes from ${crawler.url}`);
                
            } catch (fetchError: any) {
                // Ïã§Ï†ú fetch Ïã§Ìå® Ïãú Îç∞Î™®Ïö© mock Îç∞Ïù¥ÌÑ∞ ÏÇ¨Ïö©
                console.log(`[CRAWL] Real fetch failed (${fetchError.message}), using demo content`);
                realContent = this.generateDemoContent(crawler.name, crawler.url);
                pagesFound = 1;
            }
            
            // ==== PARSE WITH READABILITY FOR CLEAN CONTENT ====
            let cleanContent = realContent;
            let articleTitle = crawler.name;
            let textContent = '';
            
            try {
                const dom = new JSDOM(realContent, { url: crawler.url });
                const reader = new Readability(dom.window.document);
                const article = reader.parse();
                
                if (article) {
                    articleTitle = article.title || crawler.name;
                    cleanContent = article.content || realContent;
                    textContent = article.textContent || '';
                    console.log(`[CRAWL] Readability extracted: "${articleTitle}" (${textContent.length} chars)`);
                } else {
                    console.log('[CRAWL] Readability could not parse content, using raw HTML');
                }
            } catch (readabilityError: any) {
                console.log(`[CRAWL] Readability error: ${readabilityError.message}, using raw HTML`);
            }
            
            // Create ExtractionSource with cleaned content
            const source = await this.prisma.extractionSource.create({
                data: {
                    type: 'URL',
                    content: cleanContent,
                    metadata: { 
                        url: crawler.url, 
                        crawlerName: crawler.name,
                        articleTitle: articleTitle,
                        crawledAt: new Date().toISOString(),
                        contentLength: cleanContent.length,
                        textLength: textContent.length,
                        pagesFound,
                        parsedWithReadability: !!textContent
                    }
                }
            });
            sourceId = source.id;
            
            // ==== CREATE EXTRACTION JOB ====
            const job = await this.prisma.extractionJob.create({
                data: {
                    sourceId: source.id,
                    status: 'COMPLETED',
                    progress: 100
                }
            });
            
            // ==== EXTRACT REQUIREMENTS FROM CONTENT ====
            let extractedItems: Array<{ title: string; text: string; description: string; confidence: number }>;
            
            // Get category from crawler (cast to any to handle schema sync)
            const crawlerCategory = (crawler as any).category || 'GENERAL';
            
            // Check if this is an RSS feed (by category or content detection)
            if (crawlerCategory === 'NEWS' || this.isRssFeed(realContent)) {
                console.log(`[CRAWL] Detected RSS feed, using RSS parser with AI`);
                extractedItems = await this.parseRssFeed(crawler.url, crawler.name);
            } else {
                // Use AI to analyze ALL other content types
                console.log(`[CRAWL] Using AI to analyze ${crawlerCategory} content`);
                try {
                    const aiResult = await this.aiService.convertContentToRequirements(
                        articleTitle || crawler.name,
                        textContent,
                        crawlerCategory
                    );
                    
                    // Convert AI results to extractedItems format
                    extractedItems = (aiResult.requirements || []).map(req => ({
                        title: req.title,
                        text: req.content,
                        description: `[${req.type}][${req.priority}][${req.category}] ${req.content}\n\nüí° Í∑ºÍ±∞: ${req.rationale}\nüìä Ï∂úÏ≤ò: ${crawlerCategory}`,
                        confidence: aiResult.relevanceScore || 0.7
                    }));
                    
                    console.log(`[CRAWL] AI extracted ${extractedItems.length} requirements`);
                } catch (aiError: any) {
                    console.warn(`[CRAWL] AI analysis failed: ${aiError.message}, falling back to HTML extraction`);
                    extractedItems = this.extractRequirementsFromHtml(realContent, crawler.name);
                }
            }
            
            itemsExtracted = extractedItems.length;
            
            // Create RequirementDrafts from extracted items
            for (const item of extractedItems) {
                await this.prisma.requirementDraft.create({
                    data: {
                        jobId: job.id,
                        sourceId: source.id,
                        title: item.title,
                        content: item.description,
                        originalText: item.text,
                        confidence: item.confidence,
                        status: 'PENDING'
                    }
                });
            }
            
            // Update job with result
            await this.prisma.extractionJob.update({
                where: { id: job.id },
                data: { result: { extractedCount: itemsExtracted, crawlerName: crawler.name } }
            });
            
            status = 'SUCCESS';
            duration = Date.now() - startTime.getTime();
            console.log(`[CRAWL] Extracted ${itemsExtracted} requirement drafts from ${crawler.name}`);
            
        } catch (err: any) {
            status = 'FAILED';
            errorMessage = err.message || 'Unknown error';
            duration = Date.now() - startTime.getTime();
            console.error(`[CRAWL] Failed: ${errorMessage}`);
        }
        
        // Create history record
        const history = await this.prisma.crawlHistory.create({
            data: {
                crawlerId: id,
                status,
                duration,
                pagesFound,
                itemsExtracted,
                errorMessage,
                startedAt: startTime,
                completedAt: new Date()
            }
        });
        
        // Update crawler stats
        await this.prisma.crawler.update({
            where: { id },
            data: {
                lastRunAt: new Date(),
                successCount: status === 'SUCCESS' 
                    ? { increment: 1 } 
                    : undefined,
                errorCount: status === 'FAILED' 
                    ? { increment: 1 } 
                    : undefined
            }
        });
        
        console.log(`[CRAWL] Completed: ${crawler.name} - ${status} (${duration}ms, ${itemsExtracted} items)`);
        
        return {
            success: status === 'SUCCESS',
            historyId: history.id,
            duration,
            pagesFound,
            itemsExtracted,
            errorMessage,
            message: status === 'SUCCESS' 
                ? `${crawler.name} ÏàòÏßë ÏôÑÎ£å: ${itemsExtracted}Í±¥ Ï∂îÏ∂ú` 
                : `${crawler.name} ÏàòÏßë Ïã§Ìå®: ${errorMessage}`
        };
    }
    
    private generateDemoContent(name: string, url: string): string {
        const regulations = [
            { title: 'Ï†ú1Ï°∞ (Î™©Ï†Å)', content: 'Ïù¥ Î≤ïÏùÄ Ï†ïÎ≥¥ÌÜµÏã†ÎßùÏùò Ïù¥Ïö©ÏùÑ Ï¥âÏßÑÌïòÍ≥† Ï†ïÎ≥¥ÌÜµÏã†ÏÑúÎπÑÏä§Î•º Ïù¥Ïö©ÌïòÎäî ÏûêÏùò Í∞úÏù∏Ï†ïÎ≥¥Î•º Î≥¥Ìò∏Ìï®ÏùÑ Î™©Ï†ÅÏúºÎ°ú ÌïúÎã§.' },
            { title: 'Ï†ú2Ï°∞ (Ï†ïÏùò)', content: '"Í∞úÏù∏Ï†ïÎ≥¥"ÎûÄ ÏÉùÏ°¥ÌïòÎäî Í∞úÏù∏Ïóê Í¥ÄÌïú Ï†ïÎ≥¥Î°úÏÑú ÏÑ±Î™Ö, Ï£ºÎØºÎì±Î°ùÎ≤àÌò∏ Îì±Ïóê ÏùòÌïòÏó¨ ÎãπÌï¥ Í∞úÏù∏ÏùÑ ÏïåÏïÑÎ≥º Ïàò ÏûàÎäî Ï†ïÎ≥¥Î•º ÎßêÌïúÎã§.' },
            { title: 'Ï†ú3Ï°∞ (Í∞úÏù∏Ï†ïÎ≥¥Ïùò ÏàòÏßë)', content: 'Ï†ïÎ≥¥ÌÜµÏã†ÏÑúÎπÑÏä§ Ï†úÍ≥µÏûêÎäî ÏÑúÎπÑÏä§ Ï†úÍ≥µÏóê ÌïÑÏöîÌïú ÏµúÏÜåÌïúÏùò Í∞úÏù∏Ï†ïÎ≥¥ÎßåÏùÑ ÏàòÏßëÌïòÏó¨Ïïº ÌïúÎã§.' },
            { title: 'Ï†ú4Ï°∞ (Í∞úÏù∏Ï†ïÎ≥¥Ïùò Ïù¥Ïö©)', content: 'ÏàòÏßëÎêú Í∞úÏù∏Ï†ïÎ≥¥Îäî ÏàòÏßë Î™©Ï†Å Î≤îÏúÑ ÎÇ¥ÏóêÏÑúÎßå Ïù¥Ïö©ÌïòÏó¨Ïïº ÌïòÎ©∞, Ï†ú3ÏûêÏóêÍ≤å Ï†úÍ≥µÌï† Ïàò ÏóÜÎã§.' },
            { title: 'Ï†ú5Ï°∞ (Î≥¥ÏïàÏ°∞Ïπò)', content: 'Ï†ïÎ≥¥ÌÜµÏã†ÏÑúÎπÑÏä§ Ï†úÍ≥µÏûêÎäî Í∞úÏù∏Ï†ïÎ≥¥Ïùò Î∂ÑÏã§¬∑ÎèÑÎÇú¬∑ÎàÑÏ∂ú¬∑Î≥ÄÏ°∞ Î∞©ÏßÄÎ•º ÏúÑÌïòÏó¨ Í∏∞Ïà†Ï†Å¬∑Í¥ÄÎ¶¨Ï†Å Î≥¥Ìò∏Ï°∞ÏπòÎ•º ÌïòÏó¨Ïïº ÌïúÎã§.' }
        ];
        
        const selectedRegs = regulations.slice(0, 3 + Math.floor(Math.random() * 2));
        const contents = selectedRegs.map(r => `
            <div class="article" data-type="requirement">
                <h3>${r.title}</h3>
                <p>${r.content}</p>
            </div>
        `).join('');
        
        return `<!DOCTYPE html>
<html lang="ko">
<head><meta charset="UTF-8"><title>${name}</title></head>
<body>
    <header>
        <h1>${name}</h1>
        <p class="source">Ï∂úÏ≤ò: ${url}</p>
        <p class="date">ÏàòÏßëÏùºÏãú: ${new Date().toLocaleString('ko-KR')}</p>
    </header>
    <main>
        ${contents}
    </main>
</body>
</html>`;
    }
    
    private extractRequirementsFromHtml(html: string, crawlerName: string): Array<{
        title: string;
        text: string;
        description: string;
        confidence: number;
    }> {
        const results: Array<{ title: string; text: string; description: string; confidence: number }> = [];
        
        // Extract from <div class="article"> or <article> tags
        const articleRegex = /<(?:div[^>]*class="[^"]*article[^"]*"|article)[^>]*>([\s\S]*?)<\/(?:div|article)>/gi;
        const titleRegex = /<h[1-6][^>]*>(.*?)<\/h[1-6]>/i;
        const contentRegex = /<p[^>]*>(.*?)<\/p>/gi;
        
        let match;
        while ((match = articleRegex.exec(html)) !== null) {
            const articleHtml = match[1];
            const titleMatch = titleRegex.exec(articleHtml);
            const title = titleMatch ? titleMatch[1].replace(/<[^>]*>/g, '').trim() : `${crawlerName} ÏöîÍ±¥`;
            
            const paragraphs: string[] = [];
            let pMatch;
            while ((pMatch = contentRegex.exec(articleHtml)) !== null) {
                const text = pMatch[1].replace(/<[^>]*>/g, '').trim();
                if (text.length > 10) paragraphs.push(text);
            }
            
            if (paragraphs.length > 0) {
                results.push({
                    title: title,
                    text: paragraphs.join(' '),
                    description: `${crawlerName}ÏóêÏÑú ÏûêÎèô Ï∂îÏ∂úÎêú ÏöîÍ±¥ÏûÖÎãàÎã§. ÏõêÎ¨∏: "${paragraphs[0].slice(0, 100)}..."`,
                    confidence: 0.7 + Math.random() * 0.25
                });
            }
        }
        
        // If no articles found, try to extract from headings
        if (results.length === 0) {
            const headingRegex = /<h[1-3][^>]*>(.*?)<\/h[1-3]>/gi;
            while ((match = headingRegex.exec(html)) !== null) {
                const title = match[1].replace(/<[^>]*>/g, '').trim();
                if (title.length > 5 && title.length < 100) {
                    results.push({
                        title: title,
                        text: title,
                        description: `${crawlerName}ÏóêÏÑú Ï∂îÏ∂úÎêú Ï†úÎ™© Í∏∞Î∞ò ÏöîÍ±¥`,
                        confidence: 0.5 + Math.random() * 0.2
                    });
                }
            }
        }
        
        return results.slice(0, 10); // Max 10 items per crawl
    }
    
    private getRandomError(): string {
        const errors = [
            'Connection timeout after 30 seconds',
            'HTTP 503 Service Unavailable',
            'SSL certificate verification failed',
            'Rate limit exceeded (429)',
            'Page structure changed - selectors not found'
        ];
        return errors[Math.floor(Math.random() * errors.length)];
    }
    
    // ==== RSS FEED PARSING WITH AI CONVERSION ====
    private async parseRssFeed(url: string, crawlerName: string): Promise<Array<{
        title: string;
        text: string;
        description: string;
        confidence: number;
    }>> {
        const parser = new RSSParser();
        const results: Array<{ title: string; text: string; description: string; confidence: number }> = [];
        
        try {
            console.log(`[RSS] Parsing feed from ${url}`);
            const feed = await parser.parseURL(url);
            
            console.log(`[RSS] Found ${feed.items?.length || 0} items in "${feed.title}"`);
            
            // Process first 5 items with AI (limit to avoid timeout)
            for (const item of (feed.items || []).slice(0, 5)) {
                const newsTitle = item.title || 'Ï†úÎ™© ÏóÜÏùå';
                const newsContent = item.contentSnippet || item.content || item.summary || '';
                const link = item.link || '';
                const pubDate = item.pubDate || item.isoDate || '';
                
                try {
                    // AI converts news to IT requirements
                    const aiResult = await this.aiService.convertNewsToRequirements(newsTitle, newsContent);
                    
                    // Add AI-extracted requirements
                    for (const req of aiResult.requirements || []) {
                        results.push({
                            title: req.title,
                            text: req.content,
                            description: `[${req.type}][${req.priority}] ${req.content}\n\nüì∞ Ï∂úÏ≤ò: ${newsTitle}\nüîó ${link}\nüìÖ ${pubDate}\n\nüí° Í∑ºÍ±∞: ${req.rationale}`,
                            confidence: aiResult.relevanceScore || 0.8
                        });
                    }
                    
                    // If no requirements extracted but high relevance, add raw news
                    if ((aiResult.requirements?.length || 0) === 0 && aiResult.relevanceScore > 0.3) {
                        results.push({
                            title: `[Îâ¥Ïä§] ${newsTitle}`,
                            text: aiResult.summary || newsContent,
                            description: `[NEWS] ${crawlerName}\nÏõêÎ¨∏: ${link}\nÎ∞úÌñâÏùº: ${pubDate}\n\n${aiResult.summary || newsContent.slice(0, 300)}`,
                            confidence: aiResult.relevanceScore
                        });
                    }
                } catch (aiError: any) {
                    console.warn(`[RSS] AI conversion failed for "${newsTitle}": ${aiError.message}`);
                    // Fallback: add raw news item without AI processing
                    results.push({
                        title: newsTitle,
                        text: newsContent,
                        description: `[${crawlerName}] ${newsTitle}\nÏõêÎ¨∏: ${link}\nÎ∞úÌñâÏùº: ${pubDate}\n\n${newsContent.slice(0, 300)}`,
                        confidence: 0.5
                    });
                }
            }
        } catch (error: any) {
            console.error(`[RSS] Parse error: ${error.message}`);
        }
        
        console.log(`[RSS] Extracted ${results.length} IT requirements from news feed`);
        return results;
    }
    
    private isRssFeed(content: string): boolean {
        // Check if content is RSS/Atom XML
        return content.includes('<rss') || 
               content.includes('<feed') || 
               content.includes('<channel>') ||
               content.includes('xmlns:atom');
    }
}
